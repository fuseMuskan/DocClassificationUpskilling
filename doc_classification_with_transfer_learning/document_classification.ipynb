{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-abqhvru7SaV"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/DocumentUpskilling/dataset/documents\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "jTudc__D78im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import cv2"
      ],
      "metadata": {
        "id": "FHcyJZqi7qoz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"squeezenet\"\n",
        "\n",
        "num_classes = 4\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "feature_extract = True"
      ],
      "metadata": {
        "id": "GF02Cd4jtRgy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "hBqbNn-0xzQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'test':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "metadata": {
        "id": "_5L1Jp-LF2Q1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "k3yjkEGqHYLp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6HAiTkOHaK6",
        "outputId": "bfcac7d4-237e-4c44-911f-f9acd4c6d36e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n",
            "100%|██████████| 4.78M/4.78M [00:00<00:00, 38.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SqueezeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): Fire(\n",
            "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Fire(\n",
            "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Fire(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (7): Fire(\n",
            "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Fire(\n",
            "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Fire(\n",
            "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (12): Fire(\n",
            "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Conv2d(512, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "TgPk2gWIHeh5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1vQYARnHwWV",
        "outputId": "7632a16e-9409-426c-caa8-0220af9baed1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "\t classifier.1.weight\n",
            "\t classifier.1.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms[x]) for x in ['train', 'test']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcAEiHU01zpX",
        "outputId": "7adfddb0-c45d-4ced-abd9-5efcf5a5ab2f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming image_datasets is already defined as in your code\n",
        "for phase, dataset in image_datasets.items():\n",
        "    print(f\"Class names in {phase} set: {dataset.classes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqB7kMIbr2m1",
        "outputId": "afcb156f-9e3d-4f26-8e3f-6a341d9de096"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class names in train set: ['citizenship', 'license', 'others', 'passport']\n",
            "Class names in test set: ['citizenship', 'license', 'others', 'passport']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=10, is_inception=(model_name==\"inception\"))"
      ],
      "metadata": {
        "id": "NixBoEmKHzLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79a3c76-6be0-42f7-9607-f10671050d9a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 1.5644 Acc: 0.2931\n",
            "test Loss: 1.0737 Acc: 0.5000\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "train Loss: 1.0216 Acc: 0.5862\n",
            "test Loss: 0.9188 Acc: 0.6250\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "train Loss: 0.9517 Acc: 0.6379\n",
            "test Loss: 0.6567 Acc: 0.7500\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "train Loss: 0.8461 Acc: 0.6897\n",
            "test Loss: 0.5956 Acc: 0.7500\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "train Loss: 0.6340 Acc: 0.7931\n",
            "test Loss: 0.4887 Acc: 0.8750\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "train Loss: 0.5104 Acc: 0.7931\n",
            "test Loss: 0.3434 Acc: 0.8750\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "train Loss: 0.4709 Acc: 0.7931\n",
            "test Loss: 0.3106 Acc: 0.8750\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n",
            "train Loss: 0.4361 Acc: 0.8793\n",
            "test Loss: 0.2105 Acc: 0.9375\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "train Loss: 0.6516 Acc: 0.7586\n",
            "test Loss: 0.3069 Acc: 0.8750\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n",
            "train Loss: 0.3486 Acc: 0.8621\n",
            "test Loss: 0.2595 Acc: 0.9375\n",
            "\n",
            "Training complete in 1m 21s\n",
            "Best val Acc: 0.937500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export as onnx model"
      ],
      "metadata": {
        "id": "kJ5aWVcsjcnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxscript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a_BENRqjfwc",
        "outputId": "35302562-03d7-44b7-e26b-7ad7518dbd43"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.10/dist-packages (0.1.0.dev20240117)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.23.5)\n",
            "Requirement already satisfied: onnx>=1.14 in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.5.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.14->onnxscript) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_model = model_ft\n",
        "torch_input = torch.randn(1, 3, 224, 224)\n",
        "onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwWr88IEjlOm",
        "outputId": "75d88c9d-8b71-4364-b5b1-579af33d7d46"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_program.save(\"doc_classifier.onnx\")"
      ],
      "metadata": {
        "id": "rJ64L_PpkBvc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "onnx_model = onnx.load(\"doc_classifier.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "MimrHRb8kEgs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference using onnx model"
      ],
      "metadata": {
        "id": "hOb-yN3exiRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8nWMJQylhkE",
        "outputId": "ff527d46-08ba-41f6-d9d3-649af9a56ad0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.16.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IvBfr4KTnIsX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_using_onnx(input_img):\n",
        "\n",
        "  onnx_input = onnx_program.adapt_torch_inputs_to_onnx(input_img)\n",
        "  ort_session = onnxruntime.InferenceSession(\"doc_classifier.onnx\", providers=['CPUExecutionProvider'])\n",
        "\n",
        "  def to_numpy(tensor):\n",
        "      return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "  onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
        "\n",
        "  onnxruntime_outputs = ort_session.run(None, onnxruntime_input)\n",
        "\n",
        "  max_index = np.argmax(onnxruntime_outputs)\n",
        "\n",
        "  class_names =  ['citizenship', 'license', 'others', 'passport']\n",
        "\n",
        "  predicted_class = class_names[max_index]\n",
        "\n",
        "  return predicted_class"
      ],
      "metadata": {
        "id": "sdOGDYFemVNF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "def predict_image(image_path, actual_class):\n",
        "    # Load the image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Apply transformations\n",
        "    img = transform(img)\n",
        "\n",
        "\n",
        "\n",
        "    # Add batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "    # Move the image to the appropriate device (CPU or GPU)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    img = img.to(device)\n",
        "\n",
        "    onnx_output = predict_using_onnx(img)\n",
        "\n",
        "    return onnx_output, img\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/drive/MyDrive/DocumentUpskilling/dataset/blurred_citizenship/test/blur/CpvQJpwWIAAGJEd.jpg\"\n",
        "predicted_class,img = predict_image(image_path, actual_class=\"citizenship\")"
      ],
      "metadata": {
        "id": "1vLHCG4R1uvI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_class)"
      ],
      "metadata": {
        "id": "rktrdHwLR3q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17718baa-2c34-48d1-cf35-f932707a4b9c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "citizenship\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DocumentUpskilling/test_scripts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG4-pFugEwDl",
        "outputId": "cd1a8f75-cc19-404e-ba34-32ad57738e12"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DocumentUpskilling/test_scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile doc_classify.py\n",
        "\"\"\"\n",
        "Script for classifying images using an ONNX model.\n",
        "\n",
        "Usage:\n",
        "python doc_classify.py <model_path> <image_path>\n",
        "\n",
        "Arguments:\n",
        "- model_path (str): Path to the ONNX model file.\n",
        "- image_path (str): Path to the image to be classified.\n",
        "\n",
        "Example:\n",
        "python doc_classify.py model.onnx image.jpg\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "def transform_image(image_path):\n",
        "    \"\"\"\n",
        "    Preprocesses the input image for classification.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Preprocessed image tensor.\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Apply transformations\n",
        "    img = transform(img)\n",
        "\n",
        "    # Add batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "    # Move the image to the appropriate device (CPU or GPU)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    img = img.to(device)\n",
        "    return img\n",
        "\n",
        "def classify_image(model_path, image_path):\n",
        "    \"\"\"\n",
        "    Classifies the input image using the specified ONNX model.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the ONNX model file.\n",
        "        image_path (str): Path to the image to be classified.\n",
        "\n",
        "    Returns:\n",
        "        str: Predicted class label.\n",
        "    \"\"\"\n",
        "    # load model\n",
        "    session = onnxruntime.InferenceSession(model_path, None)\n",
        "\n",
        "    # get input name from the model\n",
        "    input_name = session.get_inputs()[0].name\n",
        "\n",
        "    # get output name from the model\n",
        "    output_name = session.get_outputs()[0].name\n",
        "\n",
        "    # transform image for the model\n",
        "    img = transform_image(image_path)\n",
        "\n",
        "    # convert image into list\n",
        "    data = json.dumps({'data': img.tolist()})\n",
        "    data = np.array(json.loads(data)['data']).astype('float32')\n",
        "\n",
        "    # feed the image into the model\n",
        "    result = session.run([output_name], {input_name: data})\n",
        "    max_index = np.argmax(result)\n",
        "    class_names = ['citizenship', 'license', 'others', 'passport']\n",
        "\n",
        "    predicted_class = class_names[max_index]\n",
        "    return predicted_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqsMWEtgB5Kk",
        "outputId": "c7a02925-5faf-4e05-8d0e-bb9f74cff31c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing doc_classify.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DocumentUpskilling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbVJ7jGpsYXj",
        "outputId": "0d9f4c50-a88e-4f8b-9c03-63e813ef6448"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DocumentUpskilling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from test_scripts.doc_classify import classify_image"
      ],
      "metadata": {
        "id": "VnPwP-J6FbXT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = classify_image(\"/content/doc_classifier.onnx\", \"/content/drive/MyDrive/DocumentUpskilling/dataset/blurred_citizenship/test/blur/CpvQJpwWIAAGJEd.jpg\")"
      ],
      "metadata": {
        "id": "6vQiPEOpBaPk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WZwDXeSIBImZ",
        "outputId": "eb16da74-2d81-4f03-cbd8-8ae7a733fc63"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'citizenship'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}